{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4jN9p5R96Qy",
    "outputId": "e5b5d0ec-dd02-4383-da60-238bac17e947"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1rFP3Y5H-_M5",
    "outputId": "f38d1e99-a79b-4454-8c2e-92ee3572c8bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "bert_model = \"bert-base-uncased\"\n",
    "roberta_model = \"roberta-base\"\n",
    "bart_model = \"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862,
     "referenced_widgets": [
      "0a892c00ab65417b837c4f54b2184ec6",
      "dfec893881da4c0eb9c51fba7e27afd1",
      "468e1ea06e1e4ffca7679be28d783e88",
      "d05337a719fd4354be09b5a527d4b6fa",
      "22dbd2ed2fb9469dab1662739a92dd1c",
      "92be40716adf474bb97f469dcc421006",
      "3346e499cdb14f58a0982261a60095a7",
      "4c5935ab4eea4f2bbe2fd4962ac82613",
      "053b714fcce049a48f758feb82db8b21",
      "c58254d2fac3449b8f12ff60e58727cf",
      "f8812f08842646ff8a8430f7c3a734f2",
      "661842f45c1f462aa42cfc2307f51bd8",
      "3092b3c880754466adbecb41becd1169",
      "0dc2e970d0f24477b312bec96b1b1448",
      "3324c5176b2c48a1856a6bab8b00af08",
      "1029860df48945b3b1ed72a897d20699",
      "76ebbaad67964dd19264a20d5c865a5a",
      "ac016656a8874c4c981ed1cac148c7b7",
      "09d57db18c314486a4ec466d0a7d3192",
      "aeb4c0e4b71c4b3fb01db429d0fa8702",
      "7d0fc60db5244b4eba2eea7d0fd91d4f",
      "685d36d9d1094ebebbe90dfee1d72bb9",
      "2e0aaf84fa81424faf214b04adce7d1c",
      "4306dca273a04b62aa30d5d57a8a209f",
      "c63339bc741c4726b337f5a04e421942",
      "053eabe0af364885bc02152642a4f31c",
      "68820b5630f041f1a70b8f64ef4a96bc",
      "993f58ed0dc74114ba402ea3b2a9ae70",
      "6a7537f6bb564d6db5c8aab928de4ecc",
      "9537cd937ae441289ff1077e3f957411",
      "b1d0d5cde9ab4d2e8f7ac5c91e70f4ef",
      "b19d182143bb4ea2aea85481cc0bb804",
      "be14fb3d960c402ebaeda6aeaefe3129",
      "15a0c6aeede94f96abdf7be4ea861037",
      "0f6f48d99c254a2398c75dc5fdaf8871",
      "b7a90203e71042c6936901c20375b325",
      "6344287049924bdea56c380298905d5c",
      "96fdb819813941dcb1763fc62fb1b46c",
      "aa19e9c38daa45ad91b4c3184ced82d1",
      "299a8e6e5a1647e1be626d33099749c9",
      "c0a6415a330e418cb5afcefa0db6f628",
      "ba6462385482482ab8556e1575dd9422",
      "37bd2bae80844de096d0fd76e8af8d52",
      "275e4b061f1941a1a44c2cae1ca584f5",
      "f3e36387a2b64855924549b83899e76b",
      "a59cb948c353466dab72ec1985ee5cb8",
      "93e2e184b86f49c08641612d781951c6",
      "350cd7049cba4b57924386397bc7c263",
      "e54bfed73ef4494cb686bfec6edd4e11",
      "1f7dad03cf5642d1ab45be5df386e1f9",
      "3f6b6f6ec82a4e05a5a857b5ec14e205",
      "8faf4c5a6682433786a0c07811942210",
      "0f4bf68a2c024329a765a032ca30bfad",
      "aff2884a7c1c41189a12ae3446508d50",
      "3494ee832b07425f94dab9ece29700df",
      "1bddd77fbcf1410e8a10da8c9da6a52d",
      "19dd3c15b9cd4633b16163795c62110d",
      "ac31830025194581a84d905e5dbaa383",
      "79c14a5627644f7d91e175a565addcec",
      "0c21305fab474898adb4c0e7f2426370",
      "29d5608e5f2c46c4b86df07252821354",
      "4b04712935c544e68fffa6036f345e63",
      "a158780c997a41cdb064a1c8084dcd8b",
      "48650e2d0460442eac614d4e5d6fe254",
      "bc6a9f4e10144cf3acb44b4d7c9e3fae",
      "e969dc05a4c04bddb6731ac49c466e31",
      "65293d1c0e09442d8b39d8352b5010d7",
      "8124c47db8be4e6cb8c21a5daf55a4bd",
      "3f55859cc90349cfbfdffbe6d0057766",
      "2bdfe2c460d34febae34662d969b863b",
      "3076948eef6b47b5ad5082c30215793d",
      "828d0f44dfcf474ba9e162754f591e24",
      "863655cbcdd94b34aa42f44d9c1ed78d",
      "1abe29a8cdf0483ea106cde5d2123bd4",
      "7f0ca1ca6e454a3e80d01f19fb69edbb",
      "d0d8f6b273e34beda6c7b3150bd4ec43",
      "e895616d0e034fc7a9ffa499a7993fb4",
      "169e30e7586f4aac8bc9babed25a37bf",
      "ddbd89ea816548dfa5bbbf14f17c2617",
      "c72f3415e52c450d8fc2a2b5f0c6efa2",
      "f3a86cfe6267446ea881cf01759e051d",
      "a5ae2e4c15cc42258d6583bed41d210a",
      "a2628eea316e44a7b17849361da06ca2",
      "1c3f359bf15e4667b2f84327870feeaa",
      "a6f8cf4f855a4efc9d295ec86c8b8f7f",
      "0d70255dc2664e07b59eb0ab9b7d750b",
      "cf930453e4054d48bc7b31bb2888be53",
      "649f367f2bb54baf9fd4e7e3ed44b443",
      "d6b4d4e269c04af187ebf83d07ca79db",
      "de05a87410084f05b23b948e304848ca",
      "b460e909362843e0907ee94699d7816e",
      "c57eb8e3b48e46d594a975e4b1165a46",
      "99ef76de3d684fc2b847283ab885f4af",
      "9e4ac61c195644b1901b8d7394a3f3e6",
      "09f2bbdf94d84a539ed34c296cd12605",
      "63bcc14e1267444db13c21d84c634aff",
      "ed95b95b5d9d44488daf1453fd01d40f",
      "d2f15436b072496f87870a91afb2ed6c",
      "dc9c0313252a446691abd5184915bd21",
      "054039f1dcad4a00b8749a67d3eb8ef1",
      "216e2f77443347f2bad4f30d6de663df",
      "393050087550482c9cd4425512e8e18f",
      "8993d67b72e34a9199a6d226f541a1fe",
      "f6adf2f5dd9e4380ad533c91ee8396d8",
      "7320396f95674b52be532f763887e0fd",
      "fda7a847c83a48bbaab04dc302d14c46",
      "ca21ee60eec24bde9ed50991dee8a744",
      "0187d07256d247b38a1cd0b94cf5a272",
      "51e9d92741b44182adbdeaaddd3dad6d",
      "09593fdd47d74c0187293a1563356b3c",
      "e4a4471163f24db08a91a08795c76d01",
      "767d0685cee24e129ad58444fc99edf1",
      "315ac3974aac473391b8f7bf56411de9",
      "ca1117d7ecb8425c9e5eca2fc9bce718",
      "c808b49802154c9abf5f446d41b4a87e",
      "59c20d76f34345469381ff6c9a3f5a07",
      "f4d3c9a90acf4d018b22f698feb2896e",
      "af9a9a2a29fa4f3c9d69f01a4848135a",
      "8dc40fc5b07b4862913af3ffb62dcd1c",
      "b8f5aba4a5324112895c9f489f3a853f",
      "d8ffb93ff71641a9a11c16d256fb0787",
      "3f43a6c3f00f45a4a91bde3b1a580189",
      "b6df280634d04b9192a4533fcc97e2bf",
      "977d5f9e162644bb95c588794f29a272",
      "25e0495fc5dd493bbff850278daadfca",
      "72246457a6aa4a9ba9ce719e502c916e",
      "46c28d7d3a0b4d4fa73356d714168e3e",
      "93d29f04a0ce4457b8cb4d88edd6642f",
      "b6d6831fb8ed42df945817ec5fb9c212",
      "5e3f06cbea6e43a99def2c3142766ee5",
      "0bd7c4976c984564a81e1e44ee1f1704",
      "a45b660321cb4973948c161e9cba6592",
      "b31a2a6de5dc4b6788b0585e1fd74676",
      "327189a4f2864fc6bedd41385bcae4d8",
      "5554a79158af42c1a241eb88b2478bd8",
      "28415c43d9e24e0c99f5b9f4433447fe",
      "c21923fc47ea4e42b2d0828a486f4fbe",
      "c3c9bd2834fc475ab53455243ef9a55d",
      "86b98e5fb3b944ebaf4e5e1165167786",
      "488c2a7b6e5944c8836bba43662e2953",
      "fe63b27c85954e99a816e20efdf60bec",
      "d591b9cce17b40e7b2bc788a6f9f3f28",
      "f6a12b76702d475090c2d8f050c40f71",
      "e19b64c985fc4553ab60f59c6a42102a",
      "976b40acc26940dfbe5f2bfd6876232f",
      "6fd00e33f6b14ce4b15bfb8a69135aa9",
      "476eee4a07354a8daa1849be2a59f8f3",
      "17cec04d200c49988c185d01b4a66157",
      "1aa3e256da9049b3943fd59a3e70eb0f",
      "456a7fbf92d9490db881c3f44ec1af31",
      "e26c1e119c484784af02d5c2f07e2106",
      "761b1f0716d74722aef3b16d2570805a",
      "89de9590a2a54b3aa72d8122a892ae01",
      "83f6b83da391436abfb5f2dc930e324c",
      "e97471c557a347b9918d29173b2be5d5",
      "061c2d8e6a8f435a97e58db948b351c9",
      "88fd8a96e7e34f3ebbf8d02cd824d655",
      "fa5085a22bdc496eb979b5a3824775b6",
      "3cae752d8a3546689ca07f47b283b2c5",
      "a932e0ffc70c4b2e8ab2bd60bca7d1dc",
      "b6c985800fd9433fb2945c5342fe319a",
      "f23a79fbaa72443ea949586e9b853044",
      "50635859e530448a94c5537a3ae854b6",
      "4377f26e53c34b839b75fe0e80edff0d",
      "1f87e8bbfbc64dc6aef92a3bd9bbe35e",
      "e140499c825a4b7d939328c277f588c3",
      "5e68fda50660479b9b961d12dd1041b3",
      "4c03678863b346d2a31ba4d096a109bc",
      "128e02b68a53472c9e9c65441c26d9e8",
      "6f2738de2f01433ba223ba7a72b9f6e3",
      "ef7242e5c953410cb1431ecf61031432",
      "051c79aa352f426d9e726c2e0fd562d2",
      "adfd0113e7d744dca347e355ab1ea45a",
      "c1a771f33aec4a2fb64e091661e9f675",
      "0f8633fcc3e748108c361ec07fe1c547",
      "33153e2f61814779971341b19fc401e9"
     ]
    },
    "id": "OiMWu0hk_Dc2",
    "outputId": "ef8ead54-49b4-4d51-d764-0e64857071d4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a892c00ab65417b837c4f54b2184ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661842f45c1f462aa42cfc2307f51bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0aaf84fa81424faf214b04adce7d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a0c6aeede94f96abdf7be4ea861037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e36387a2b64855924549b83899e76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bddd77fbcf1410e8a10da8c9da6a52d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65293d1c0e09442d8b39d8352b5010d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169e30e7586f4aac8bc9babed25a37bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b4d4e269c04af187ebf83d07ca79db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054039f1dcad4a00b8749a67d3eb8ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a4471163f24db08a91a08795c76d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f43a6c3f00f45a4a91bde3b1a580189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31a2a6de5dc4b6788b0585e1fd74676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19b64c985fc4553ab60f59c6a42102a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97471c557a347b9918d29173b2be5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e140499c825a4b7d939328c277f588c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The future of Artificial Intelligence is'}]\n",
      "[{'generated_text': 'The future of Artificial Intelligence is outspokenrame seventhValues Haas Unsure Side accur Camelvertinggars lucrative declNap pilgrims pilgrims pilgrims Sidegars constitutionallygars]} Floating pristine doseensibly escaped fruit fraught finest Side fruit pristineש decorations Siderame fruit decorations scary scary fruitagy Winner eh invitation improvised diamensibly Recordingensibly eh eh Schwe improvised improvised dose wiserameMeshMesh eh therapists Recording doseensibly twilight twilight scary improvised dose Camel toddler twilight twilight specialist Side semantic Newark trace Any eh eh gloom matter semantic semanticrame Mead improvised lucrative武 improvisedPi improvised semantic eh dose eh eh risk Consequentlyrame eh improvised improvised improvisedormonal improvised actively semanticPiPiPi dose ehrameramePi improvised brewing matter semantic improvised toddlerrameheaderrame improvised documented gloomheaderheaderrame ehNap doseHarry semantic improvised Meadheaderheader matterrame invitationlocation improvised fraught documented dose documentedlocation improvised improvisedPiheader semantic semantic fraught improvised \"- improvisedlocation gloom semantic improvised shudder semantic Wash \"- specialist improvisedlocation improvised semanticPiHarryÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ improvised improvised gloom Hawai semanticheader semantic matterrame handling RedskinsPi semantic improvised semantic traceensibly semantic accur accur risk invitation semantic improvised improvised Redskins semantic semanticheader specialist \"-rame feder improvised dose specialist documented semantic specialist specialist arrestingrame semantic aptheader semanticheader gloom invitation specialist improvisedensibly semantic gloom improvised improvised Padres specialist Hawai documentedheader feder invitation Redskins improvisedheader semantic arrestingrame risk semantic semantic semanticcause semanticheader'}]\n"
     ]
    }
   ],
   "source": [
    "gen_bert = pipeline(\"text-generation\", model=bert_model)\n",
    "gen_roberta = pipeline(\"text-generation\", model=roberta_model)\n",
    "gen_bart = pipeline(\"text-generation\", model=bart_model)\n",
    "\n",
    "print(gen_bert(\"The future of Artificial Intelligence is\"))\n",
    "print(gen_roberta(\"The future of Artificial Intelligence is\"))\n",
    "print(gen_bart(\"The future of Artificial Intelligence is\", max_length=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bc0wqP9w_FPI",
    "outputId": "99aff5e1-a1f4-475a-cd33-4924458bd4c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.5396932363510132, 'token': 3443, 'token_str': 'create', 'sequence': 'the goal of generative ai is to create new content.'}, {'score': 0.15575720369815826, 'token': 9699, 'token_str': 'generate', 'sequence': 'the goal of generative ai is to generate new content.'}, {'score': 0.05405500903725624, 'token': 3965, 'token_str': 'produce', 'sequence': 'the goal of generative ai is to produce new content.'}, {'score': 0.04451530799269676, 'token': 4503, 'token_str': 'develop', 'sequence': 'the goal of generative ai is to develop new content.'}, {'score': 0.01757744885981083, 'token': 5587, 'token_str': 'add', 'sequence': 'the goal of generative ai is to add new content.'}]\n",
      "[{'score': 0.3711312413215637, 'token': 5368, 'token_str': ' generate', 'sequence': 'The goal of Generative AI is to generate new content.'}, {'score': 0.3677145540714264, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}, {'score': 0.08351420611143112, 'token': 8286, 'token_str': ' discover', 'sequence': 'The goal of Generative AI is to discover new content.'}, {'score': 0.021335121244192123, 'token': 465, 'token_str': ' find', 'sequence': 'The goal of Generative AI is to find new content.'}, {'score': 0.016521666198968887, 'token': 694, 'token_str': ' provide', 'sequence': 'The goal of Generative AI is to provide new content.'}]\n",
      "[{'score': 0.07461541891098022, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}, {'score': 0.06571870297193527, 'token': 244, 'token_str': ' help', 'sequence': 'The goal of Generative AI is to help new content.'}, {'score': 0.060880109667778015, 'token': 694, 'token_str': ' provide', 'sequence': 'The goal of Generative AI is to provide new content.'}, {'score': 0.03593561053276062, 'token': 3155, 'token_str': ' enable', 'sequence': 'The goal of Generative AI is to enable new content.'}, {'score': 0.03319477662444115, 'token': 1477, 'token_str': ' improve', 'sequence': 'The goal of Generative AI is to improve new content.'}]\n"
     ]
    }
   ],
   "source": [
    "fill_bert = pipeline(\"fill-mask\", model=bert_model)\n",
    "fill_roberta = pipeline(\"fill-mask\", model=roberta_model)\n",
    "fill_bart = pipeline(\"fill-mask\", model=bart_model)\n",
    "\n",
    "print(fill_bert(\"The goal of Generative AI is to [MASK] new content.\"))\n",
    "print(fill_roberta(\"The goal of Generative AI is to <mask> new content.\"))\n",
    "print(fill_bart(\"The goal of Generative AI is to <mask> new content.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oc2UlqF-_GXc",
    "outputId": "73a6ba7f-27ba-47d2-dd99-870eca7c8df7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.009195590857416391, 'start': 0, 'end': 42, 'answer': 'Generative AI poses significant risks such'}\n",
      "{'score': 0.004472052212804556, 'start': 32, 'end': 71, 'answer': 'risks such as hallucinations, bias, and'}\n",
      "{'score': 0.04365525580942631, 'start': 32, 'end': 81, 'answer': 'risks such as hallucinations, bias, and deepfakes'}\n"
     ]
    }
   ],
   "source": [
    "qa_bert = pipeline(\"question-answering\", model=bert_model)\n",
    "qa_roberta = pipeline(\"question-answering\", model=roberta_model)\n",
    "qa_bart = pipeline(\"question-answering\", model=bart_model)\n",
    "\n",
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "\n",
    "print(qa_bert(question=\"What are the risks?\", context=context))\n",
    "print(qa_roberta(question=\"What are the risks?\", context=context))\n",
    "print(qa_bart(question=\"What are the risks?\", context=context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqk21PImAOrn"
   },
   "source": [
    "| Task       | Model   | Success / Failure | Observation                                   | Architectural Reason |\n",
    "|------------|---------|------------------|-----------------------------------------------|----------------------|\n",
    "| Generation | BERT    | Failure          | Cannot generate text                          | Encoder-only, no decoder |\n",
    "| Generation | RoBERTa | Failure          | Cannot generate text                          | Encoder-only |\n",
    "| Generation | BART    | Partial Failure  | Generates repetitive or nonsensical text      | Seq2seq forced into causal LM |\n",
    "| Fill-Mask  | BERT    | Success          | High-confidence correct predictions           | MLM-trained encoder |\n",
    "| Fill-Mask  | RoBERTa | Success          | Stronger confidence balance                   | Optimized MLM training |\n",
    "| Fill-Mask  | BART    | Partial Success  | Lower confidence predictions                  | Denoising autoencoder |\n",
    "| QA         | BERT    | Poor             | Low-confidence partial span                   | No QA fine-tuning |\n",
    "| QA         | RoBERTa | Poor             | Slightly cleaner spans                        | Better encoder representations |\n",
    "| QA         | BART    | Poor             | Best-looking span but unstable                | Not designed for extractive QA |\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
